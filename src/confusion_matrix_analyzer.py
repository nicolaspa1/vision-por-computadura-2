#!/usr/bin/env python3
"""
Analizador avanzado de Matriz de ConfusiÃ³n para detecciÃ³n de grietas
Genera anÃ¡lisis detallado del comportamiento del modelo
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
from pathlib import Path
import json


class CrackConfusionAnalyzer:
    """Analizador especializado en matrices de confusiÃ³n para grietas"""

    def __init__(self):
        self.class_names = ['Sin Grieta', 'Con Grieta']
        self.colors = {
            'TN': '#2E8B57',  # Verde oscuro - True Negative
            'TP': '#228B22',  # Verde claro - True Positive
            'FN': '#DC143C',  # Rojo oscuro - False Negative (CRÃTICO)
            'FP': '#FF6347'  # Rojo claro - False Positive (COSTOSO)
        }

    def create_detailed_confusion_matrix(self, y_true, y_pred, save_path=None):
        """Crear matriz de confusiÃ³n detallada con anÃ¡lisis"""

        # Calcular matriz de confusiÃ³n
        cm = confusion_matrix(y_true, y_pred)

        # Extraer valores
        tn, fp, fn, tp = cm.ravel()
        total = len(y_true)

        # Crear figura con subplots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('AnÃ¡lisis Completo de Matriz de ConfusiÃ³n - DetecciÃ³n de Grietas',
                     fontsize=16, fontweight='bold')

        # 1. Matriz de confusiÃ³n bÃ¡sica
        self._plot_basic_confusion_matrix(cm, axes[0, 0])

        # 2. Matriz con porcentajes
        self._plot_percentage_confusion_matrix(cm, axes[0, 1])

        # 3. AnÃ¡lisis de errores
        self._plot_error_analysis(tn, fp, fn, tp, total, axes[1, 0])

        # 4. MÃ©tricas derivadas
        self._plot_metrics_radar(tn, fp, fn, tp, axes[1, 1])

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… AnÃ¡lisis guardado: {save_path}")

        plt.show()

        # Generar reporte textual
        self._generate_textual_report(tn, fp, fn, tp, total)

        return cm, self._calculate_metrics(tn, fp, fn, tp)

    def _plot_basic_confusion_matrix(self, cm, ax):
        """Matriz de confusiÃ³n bÃ¡sica con colores por tipo de predicciÃ³n"""

        # Crear matriz de colores personalizada
        colors = np.array([[self.colors['TN'], self.colors['FP']],
                           [self.colors['FN'], self.colors['TP']]])

        # Plot con colores personalizados
        im = ax.imshow(cm, cmap='Blues', alpha=0.3)

        # AÃ±adir nÃºmeros y colores de fondo
        for i in range(2):
            for j in range(2):
                # Determinar tipo de predicciÃ³n
                if i == 0 and j == 0:
                    pred_type, color = "TN\n(Verdadero Negativo)", self.colors['TN']
                elif i == 0 and j == 1:
                    pred_type, color = "FP\n(Falso Positivo)", self.colors['FP']
                elif i == 1 and j == 0:
                    pred_type, color = "FN\n(Falso Negativo)", self.colors['FN']
                else:
                    pred_type, color = "TP\n(Verdadero Positivo)", self.colors['TP']

                # AÃ±adir rectÃ¡ngulo de color
                rect = plt.Rectangle((j - 0.4, i - 0.4), 0.8, 0.8,
                                     facecolor=color, alpha=0.7, edgecolor='black')
                ax.add_patch(rect)

                # AÃ±adir texto
                ax.text(j, i - 0.1, f'{cm[i, j]}', ha='center', va='center',
                        fontsize=20, fontweight='bold', color='white')
                ax.text(j, i + 0.2, pred_type, ha='center', va='center',
                        fontsize=8, fontweight='bold', color='white')

        ax.set_title('Matriz de ConfusiÃ³n Detallada')
        ax.set_xlabel('PredicciÃ³n')
        ax.set_ylabel('Valor Real')
        ax.set_xticks([0, 1])
        ax.set_yticks([0, 1])
        ax.set_xticklabels(self.class_names)
        ax.set_yticklabels(self.class_names)

    def _plot_percentage_confusion_matrix(self, cm, ax):
        """Matriz de confusiÃ³n con porcentajes"""

        cm_percent = cm.astype('float') / cm.sum() * 100

        sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', ax=ax,
                    xticklabels=self.class_names, yticklabels=self.class_names,
                    cbar_kws={'label': 'Porcentaje del Total'})

        ax.set_title('DistribuciÃ³n Porcentual')
        ax.set_xlabel('PredicciÃ³n')
        ax.set_ylabel('Valor Real')

    def _plot_error_analysis(self, tn, fp, fn, tp, total, ax):
        """AnÃ¡lisis detallado de errores"""

        # Datos para el grÃ¡fico
        categories = ['Predicciones\nCorrectas', 'Falsos\nPositivos', 'Falsos\nNegativos']
        values = [tn + tp, fp, fn]
        percentages = [v / total * 100 for v in values]
        colors = ['#2E8B57', '#FF6347', '#DC143C']

        # Crear barras
        bars = ax.bar(categories, percentages, color=colors, alpha=0.8, edgecolor='black')

        # AÃ±adir valores en las barras
        for bar, value, pct in zip(bars, values, percentages):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                    f'{value}\n({pct:.1f}%)', ha='center', va='bottom',
                    fontweight='bold')

        ax.set_title('AnÃ¡lisis de Errores del Modelo')
        ax.set_ylabel('Porcentaje (%)')
        ax.set_ylim(0, max(percentages) * 1.2)
        ax.grid(True, alpha=0.3)

        # AÃ±adir lÃ­nea de referencia para accuracy
        accuracy = (tn + tp) / total * 100
        ax.axhline(y=accuracy, color='green', linestyle='--', alpha=0.7,
                   label=f'Accuracy: {accuracy:.1f}%')
        ax.legend()

    def _plot_metrics_radar(self, tn, fp, fn, tp, ax):
        """GrÃ¡fico radar con mÃ©tricas clave"""

        metrics = self._calculate_metrics(tn, fp, fn, tp)

        # Preparar datos para radar
        categories = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score']
        values = [
            metrics['accuracy'],
            metrics['precision'],
            metrics['recall'],
            metrics['specificity'],
            metrics['f1_score']
        ]

        # Convertir a porcentajes
        values = [v * 100 for v in values]

        # Crear grÃ¡fico de barras horizontales (mÃ¡s claro que radar)
        y_pos = np.arange(len(categories))
        bars = ax.barh(y_pos, values, color='skyblue', alpha=0.8, edgecolor='navy')

        # Personalizar
        ax.set_yticks(y_pos)
        ax.set_yticklabels(categories)
        ax.set_xlabel('Porcentaje (%)')
        ax.set_title('MÃ©tricas de Performance')
        ax.set_xlim(0, 100)

        # AÃ±adir valores en las barras
        for bar, value in zip(bars, values):
            width = bar.get_width()
            ax.text(width + 1, bar.get_y() + bar.get_height() / 2.,
                    f'{value:.1f}%', ha='left', va='center', fontweight='bold')

        # LÃ­nea de referencia en 95%
        ax.axvline(x=95, color='green', linestyle='--', alpha=0.7, label='Objetivo 95%')
        ax.legend()

    def _calculate_metrics(self, tn, fp, fn, tp):
        """Calcular todas las mÃ©tricas relevantes"""

        accuracy = (tp + tn) / (tp + tn + fp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # MÃ©tricas especÃ­ficas para detecciÃ³n de grietas
        false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0  # Â¡CrÃ­tico!
        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'f1_score': f1_score,
            'false_negative_rate': false_negative_rate,
            'false_positive_rate': false_positive_rate,
            'true_negatives': tn,
            'false_positives': fp,
            'false_negatives': fn,
            'true_positives': tp
        }

    def _generate_textual_report(self, tn, fp, fn, tp, total):
        """Generar reporte textual detallado"""

        metrics = self._calculate_metrics(tn, fp, fn, tp)

        print("\n" + "=" * 70)
        print("ğŸ“Š REPORTE DETALLADO DE MATRIZ DE CONFUSIÃ“N")
        print("=" * 70)

        print(f"\nğŸ”¢ VALORES ABSOLUTOS:")
        print(f"   âœ… Verdaderos Negativos (TN): {tn} - Correctamente identificÃ³ SIN grieta")
        print(f"   âœ… Verdaderos Positivos (TP): {tp} - Correctamente identificÃ³ CON grieta")
        print(f"   âŒ Falsos Positivos (FP): {fp} - Dijo CON grieta, pero era SIN grieta")
        print(f"   ğŸš¨ Falsos Negativos (FN): {fn} - Dijo SIN grieta, pero era CON grieta")

        print(f"\nğŸ“Š MÃ‰TRICAS CLAVE:")
        print(f"   ğŸ¯ Accuracy: {metrics['accuracy']:.3f} ({metrics['accuracy'] * 100:.1f}%)")
        print(f"   ğŸ” Precision: {metrics['precision']:.3f} ({metrics['precision'] * 100:.1f}%)")
        print(f"   ğŸ£ Recall: {metrics['recall']:.3f} ({metrics['recall'] * 100:.1f}%)")
        print(f"   ğŸ›¡ï¸  Specificity: {metrics['specificity']:.3f} ({metrics['specificity'] * 100:.1f}%)")
        print(f"   âš–ï¸  F1-Score: {metrics['f1_score']:.3f} ({metrics['f1_score'] * 100:.1f}%)")

        print(f"\nğŸš¨ ANÃLISIS DE RIESGOS:")
        print(f"   âš ï¸  Tasa Falsos Negativos: {metrics['false_negative_rate'] * 100:.2f}%")
        print(f"       â†’ {fn} grietas reales NO detectadas de {tp + fn} totales")
        print(f"   ğŸ’° Tasa Falsos Positivos: {metrics['false_positive_rate'] * 100:.2f}%")
        print(f"       â†’ {fp} alarmas falsas de {tn + fp} superficies sanas")

        print(f"\nğŸ—ï¸  INTERPRETACIÃ“N PARA INFRAESTRUCTURA:")

        # AnÃ¡lisis de Falsos Negativos (CRÃTICO)
        if metrics['false_negative_rate'] < 0.01:  # <1%
            fn_status = "ğŸŸ¢ EXCELENTE"
            fn_msg = "Muy bajo riesgo de no detectar grietas"
        elif metrics['false_negative_rate'] < 0.05:  # <5%
            fn_status = "ğŸŸ¡ ACEPTABLE"
            fn_msg = "Riesgo moderado, monitoreo recomendado"
        else:  # >5%
            fn_status = "ğŸ”´ CRÃTICO"
            fn_msg = "Riesgo alto, requiere mejoras urgentes"

        print(f"   Falsos Negativos: {fn_status} - {fn_msg}")

        # AnÃ¡lisis de Falsos Positivos (COSTOSO)
        if metrics['false_positive_rate'] < 0.05:  # <5%
            fp_status = "ğŸŸ¢ EFICIENTE"
            fp_msg = "Pocas inspecciones innecesarias"
        elif metrics['false_positive_rate'] < 0.15:  # <15%
            fp_status = "ğŸŸ¡ MODERADO"
            fp_msg = "Costo operativo controlado"
        else:  # >15%
            fp_status = "ğŸ”´ COSTOSO"
            fp_msg = "Muchas inspecciones innecesarias"

        print(f"   Falsos Positivos: {fp_status} - {fp_msg}")

        print(f"\nğŸ’¡ RECOMENDACIONES:")
        if metrics['false_negative_rate'] > 0.02:
            print(f"   ğŸ”§ Ajustar threshold para reducir falsos negativos")
        if metrics['false_positive_rate'] > 0.10:
            print(f"   âš–ï¸  Considerar balance entre sensibilidad y especificidad")
        if metrics['accuracy'] > 0.99:
            print(f"   ğŸ‰ Modelo listo para producciÃ³n en inspecciÃ³n industrial")

        print("=" * 70)


def analyze_confusion_matrix_from_predictions(y_true, y_pred, save_dir="results/plots"):
    """FunciÃ³n principal para analizar matriz de confusiÃ³n"""

    analyzer = CrackConfusionAnalyzer()

    # Crear directorio si no existe
    Path(save_dir).mkdir(parents=True, exist_ok=True)

    # Generar anÃ¡lisis completo
    save_path = Path(save_dir) / "detailed_confusion_matrix.png"
    cm, metrics = analyzer.create_detailed_confusion_matrix(y_true, y_pred, save_path)

    # Guardar mÃ©tricas en JSON
    metrics_path = Path(save_dir).parent / "logs" / "confusion_matrix_metrics.json"
    metrics_path.parent.mkdir(parents=True, exist_ok=True)

    with open(metrics_path, 'w') as f:
        # Convertir numpy types a tipos nativos de Python para JSON
        json_metrics = {k: float(v) if isinstance(v, (np.integer, np.floating)) else v
                        for k, v in metrics.items()}
        json.dump(json_metrics, f, indent=2)

    print(f"âœ… MÃ©tricas guardadas: {metrics_path}")

    return cm, metrics


# Ejemplo de uso con datos simulados
def demo_confusion_matrix():
    """Demo con datos simulados de alta calidad"""

    print("ğŸ¬ DEMO: AnÃ¡lisis de Matriz de ConfusiÃ³n")
    print("=" * 50)

    # Simular resultados de un modelo muy bueno (como el tuyo)
    np.random.seed(42)

    n_samples = 2000
    # 99.5% accuracy
    y_true = np.random.randint(0, 2, n_samples)
    y_pred = y_true.copy()

    # Introducir algunos errores realistas
    error_indices = np.random.choice(n_samples, size=10, replace=False)
    y_pred[error_indices] = 1 - y_pred[error_indices]

    # Analizar
    cm, metrics = analyze_confusion_matrix_from_predictions(y_true, y_pred)

    return cm, metrics


if __name__ == "__main__":
    demo_confusion_matrix()